{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a normalizing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the API in `flowMC` to train two different normalizing flow networks to approximate a simple test distribution. The API is built on top of the companion libraries of Jax for deep learning, `flax` and `optax`. \n",
    "\n",
    "In typical applications of `flowMC` to obtain samples from a given posterior distribution you will not need to interact with this level of the API, the training will be directly handled within the sampling. However you will need to choose the normalizing flow model and this tutorial exemplifies the abilities of the two models currently available in the package.\n",
    "\n",
    "We train both a RealNVP flow from [[Dinh et al. 2016]](https://arxiv.org/abs/1605.08803) and a more complex normalizing flow model, the rational quadratic spline model [[Durkan et al. 2019]](https://arxiv.org/abs/1906.04032)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp  # JAX NumPy\n",
    "import jax.random as random  # JAX random\n",
    "import optax  # Optimizers\n",
    "import equinox as eqx  # Equinox\n",
    "\n",
    "\n",
    "from flowMC.nfmodel.realNVP import RealNVP\n",
    "from flowMC.nfmodel.rqSpline import MaskedCouplingRQSpline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [make_moons](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) from scikit-learn to create a toy dataset in 2-dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = jnp.array(make_moons(n_samples=100000, noise=0.05)[0])\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], s=0.5, alpha=0.5, label=\"data\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RealNVPs \n",
    "We first use the RealNVP model to fit the data. We need to specify: \n",
    "- `n_layers`: the number of coupling layers. \n",
    "- `n_hidden`: the width of the hidden layers in the 1-hidden layer MLPs for learning the scales and translations in the affine coupling layers. \n",
    "\n",
    "Inflating these numbers provides more flexibility to the normalizing flow, yet at the cost of increasing the computational budget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "n_feature = 2\n",
    "n_layers = 10\n",
    "n_hidden = 100\n",
    "\n",
    "key, subkey = jax.random.split(jax.random.PRNGKey(0), 2)\n",
    "\n",
    "model = RealNVP(\n",
    "    n_feature,\n",
    "    n_layers,\n",
    "    n_hidden,\n",
    "    subkey,\n",
    "    data_mean=jnp.mean(data, axis=0),\n",
    "    data_cov=jnp.cov(data.T),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize a `train_state` following `flax` logic and an `optax` optimizer beforw lanching the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters\n",
    "num_epochs = 100\n",
    "batch_size = 10000\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "optim = optax.adam(learning_rate)\n",
    "state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "key, subkey = jax.random.split(key)\n",
    "key, model, state, loss = model.train(key, data, optim, state, num_epochs, batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can visualize what the flow has learned by comparing the data distribution to the distribution of samples from the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key, 2)\n",
    "nf_samples = model.sample(subkey, 10000)\n",
    "plt.figure()\n",
    "plt.scatter(data[:, 0], data[:, 1], s=0.5, alpha=0.5, label=\"data\")\n",
    "plt.scatter(\n",
    "    nf_samples[:, 0], nf_samples[:, 1], s=0.5, alpha=0.5, label=\"RealNVP samples\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQSplines\n",
    "The second type of flows available are the RQSpline. These models are also based on coupling layers, however they allow for transformation more expressive than affine, namely splines of quotients of quadratic functions. Here the parameters are: \n",
    "- `n_layers`: the number of coupling layers. \n",
    "- `n_hidden`: the list of widths of the hidden layers MLPs for learning the polynomial coefficients.\n",
    "-  `n_bins`: the number of bins for the spline decompositions.\n",
    "\n",
    "As previsouly, the bigger these numbers the more flexibility to the normalizing flow and higher is the computational cost of one training iteration. While RQSplines are generally more computationally demanding per training step than RealNVPs, there can be a favorable trade-off in selecting this more sophisticated model as it may require less iterations to converge to a satisfactory solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "n_feature = 2\n",
    "n_layers = 8\n",
    "n_hiddens = [64, 64]\n",
    "n_bins = 8\n",
    "\n",
    "key, subkey = jax.random.split(jax.random.PRNGKey(1))\n",
    "\n",
    "model = MaskedCouplingRQSpline(\n",
    "    n_feature,\n",
    "    n_layers,\n",
    "    n_hiddens,\n",
    "    n_bins,\n",
    "    subkey,\n",
    "    data_cov=jnp.cov(data.T),\n",
    "    data_mean=jnp.mean(data, axis=0),\n",
    ")\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 10000\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "optim = optax.adam(learning_rate)\n",
    "state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "key, subkey = jax.random.split(key)\n",
    "key, model, state, loss = model.train(key, data, optim, state, num_epochs, batch_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key, 2)\n",
    "nf_samples = model.sample(subkey, 10000)\n",
    "plt.figure()\n",
    "plt.scatter(data[:, 0], data[:, 1], s=0.5, alpha=0.5, label=\"data\")\n",
    "plt.scatter(\n",
    "    nf_samples[:, 0], nf_samples[:, 1], s=0.5, alpha=0.5, label=\"RQSpline samples\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nfsampler')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3435402b2d55b78130a6e20f25451ea6f9b7b6d3db5230d46efc4484709d6778"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
