{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A step-by-step example running `flowMC`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will showcase the ability of ``flowMC`` to sample efficiently a 5-dimensional multi-modal distribution. We will go through the typical steps of verification one follows to confirm the quality of the samples at the end of the MCMC run. \n",
    "\n",
    "First we import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - The target distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step consists in defining the target distribution using ``jax.numpy``. This function should take as input a ``DeviceArray`` from ``jax`` representing a single realization of the variable of interest. This function will be automatically vectorized using ``jax.vmap`` in the execution of the code to handle multiple realizations at the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "n_dim = 5\n",
    "\n",
    "\n",
    "def target_dual_moon(x, data=None):\n",
    "    \"\"\"\n",
    "    Term 2 and 3 separate the distribution and smear it along the first and second dimension\n",
    "    \"\"\"\n",
    "    term1 = 0.5 * ((jnp.linalg.norm(x) - 2) / 0.1) ** 2\n",
    "    term2 = -0.5 * ((x[:1] + jnp.array([-3.0, 3.0])) / 0.8) ** 2\n",
    "    term3 = -0.5 * ((x[1:2] + jnp.array([-3.0, 3.0])) / 0.6) ** 2\n",
    "    return -(term1 - logsumexp(term2) - logsumexp(term3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple example of a 5-dimensional multi-modal distribution:\n",
    "\n",
    "\n",
    "![Dual Moon](https://github.com/kazewong/flowMC/blob/Bayeux-example/example/notebook/dual_moon.png?raw=true)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - The initial position of walkers "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to initialize the walkers by drawing realizations of a standard normal distribution. We now have to set the number of walkers ``n_chains``. \n",
    "\n",
    "Note that ``jax`` requires that random seed are explicitly defined at each stochastic step. The package provides a function ``initialize_rng_keys`` to initialize in one go all the seeds necessary for one run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chains = 20\n",
    "\n",
    "rng_key, subkey = jax.random.split(jax.random.PRNGKey(42))\n",
    "initial_position = jax.random.normal(subkey, shape=(n_chains, n_dim)) * 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - The normalizing flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``flowmc`` sampler adaptively trains a normalizing flow (NF) to become an efficient proposal for non-local Metropolis-Hastings steps. The package includes two models of normalizing flows, RealNVPs and RQSpline. Here we go for the simpler class of RQSpline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowMC.nfmodel.rqSpline import MaskedCouplingRQSpline\n",
    "\n",
    "n_dim = 5\n",
    "n_layers = 4\n",
    "hidden_size = [32, 32]\n",
    "num_bins = 8\n",
    "data = jnp.zeros(n_dim)\n",
    "\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "model = MaskedCouplingRQSpline(\n",
    "    n_dim, n_layers, hidden_size, num_bins, subkey\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Sampler initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``flowMC`` sampler combines non-local steps driven by the normalizing flow with a local sampler ensuring the MCMC progresses also in the regions that the normalizing flow has not yet learned. \n",
    "\n",
    "Here we will employ a MALA sampler, of which we need to choose the stepsize to be passed at run time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowMC.sampler.MALA import MALA\n",
    "\n",
    "step_size = 1e-1\n",
    "MALA_Sampler = MALA(target_dual_moon, True, {\"step_size\": 0.1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring the full ``flowMC`` sampler requires to specify the schedule of local sampling, global sampling with the NF proposal, and training of the NF.\n",
    "- ``n_loop_training`` and ``n_loop_production`` fixes the number of cycles of the following operations\n",
    "- ``n_local_steps`` is the number of iteration of the global sampler per loop\n",
    "- ``n_global_steps`` is the number of Metropolis-Hastings with the NF proposal steps per loop\n",
    "- ``num_epochs`` is the number of training epochs per loop\n",
    "\n",
    "The remaining parameters that can be fixed are the training hyperparameters for the NF model: \n",
    "- ``learning_rate``, ``momentum`` and ``batchsize`` for the Adam optimizer\n",
    "- ``max_samples`` which fixes the maximum number of previous walkers configuration to be used as training data\n",
    "\n",
    "The ``Sampler`` can be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowMC.sampler.Sampler import Sampler\n",
    "\n",
    "n_loop_training = 20\n",
    "n_loop_production = 20\n",
    "n_local_steps = 100\n",
    "n_global_steps = 10\n",
    "num_epochs = 5\n",
    "\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "batch_size = 5000\n",
    "max_samples = 5000\n",
    "\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "nf_sampler = Sampler(\n",
    "    n_dim,\n",
    "    subkey,\n",
    "    {'data': jnp.zeros(5)},\n",
    "    MALA_Sampler,\n",
    "    model,\n",
    "    n_loop_training=n_loop_training,\n",
    "    n_loop_production=n_loop_production,\n",
    "    n_local_steps=n_local_steps,\n",
    "    n_global_steps=n_global_steps,\n",
    "    n_chains=n_chains,\n",
    "    n_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    momentum=momentum,\n",
    "    batch_size=batch_size,\n",
    "    use_global=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready, we can launch the sampler. The first cycle of sampling-training is the slowest because of the just-in time compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_sampler.sample(initial_position, data=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Checking the results\n",
    "\n",
    "First let's look at what happened during the training run during which the global sampler was tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_train = nf_sampler.get_sampler_state(training=True)\n",
    "print(\"Logged during tuning:\", out_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = np.array(out_train[\"chains\"])\n",
    "global_accs = np.array(out_train[\"global_accs\"])\n",
    "local_accs = np.array(out_train[\"local_accs\"])\n",
    "loss_vals = np.array(out_train[\"loss_vals\"])\n",
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "nf_samples = np.array(nf_sampler.sample_flow(subkey, 1000))\n",
    "\n",
    "\n",
    "# Plot 2 chains in the plane of 2 coordinates for first visual check\n",
    "plt.figure(figsize=(6, 6))\n",
    "axs = [plt.subplot(2, 2, i + 1) for i in range(4)]\n",
    "plt.sca(axs[0])\n",
    "plt.title(\"2d proj of 2 chains\")\n",
    "\n",
    "plt.plot(chains[0, :, 0], chains[0, :, 1], \"o-\", alpha=0.5, ms=2)\n",
    "plt.plot(chains[1, :, 0], chains[1, :, 1], \"o-\", alpha=0.5, ms=2)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.sca(axs[1])\n",
    "plt.title(\"NF loss\")\n",
    "plt.plot(loss_vals.reshape(-1))\n",
    "plt.xlabel(\"iteration\")\n",
    "\n",
    "plt.sca(axs[2])\n",
    "plt.title(\"Local Acceptance\")\n",
    "plt.plot(local_accs.mean(0))\n",
    "plt.xlabel(\"iteration\")\n",
    "\n",
    "plt.sca(axs[3])\n",
    "plt.title(\"Global Acceptance\")\n",
    "plt.plot(global_accs.mean(0))\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.tight_layout()\n",
    "plt.show(block=False)\n",
    "\n",
    "labels = [\"$x_1$\", \"$x_2$\", \"$x_3$\", \"$x_4$\", \"$x_5$\"]\n",
    "# Plot all chains\n",
    "figure = corner.corner(chains.reshape(-1, n_dim), labels=labels)\n",
    "figure.set_size_inches(7, 7)\n",
    "figure.suptitle(\"Visualize samples\")\n",
    "plt.show(block=False)\n",
    "\n",
    "# Plot Nf samples\n",
    "figure = corner.corner(nf_samples, labels=labels)\n",
    "figure.set_size_inches(7, 7)\n",
    "figure.suptitle(\"Visualize NF samples\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's analyze the production run. We will first visualize the samples and acceptance of the local and global sampler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_prod = nf_sampler.get_sampler_state()\n",
    "print(\"Logged in production:\", out_prod.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = np.array(out_prod[\"chains\"])\n",
    "index = 7  # choosing random chain index to plot results\n",
    "global_accs = np.array(out_prod[\"global_accs\"])\n",
    "local_accs = np.array(out_prod[\"local_accs\"])\n",
    "log_prob_trace = np.array(out_prod[\"log_prob\"][np.array([index, index + 10]), :]).T\n",
    "\n",
    "plt.figure(figsize=(12, 3.5))\n",
    "axs = [plt.subplot(1, 4, i + 1) for i in range(4)]\n",
    "plt.sca(axs[0])\n",
    "plt.title(\"2d proj of 2 chains\")\n",
    "\n",
    "plt.plot(chains[index, :, 0], chains[index, :, 1], \"o-\", alpha=0.5, ms=2)\n",
    "plt.plot(chains[index + 10, :, 0], chains[index + 10, :, 1], \"o-\", alpha=0.5, ms=2)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "plt.sca(axs[1])\n",
    "plt.title(\"Local Acceptance\")\n",
    "plt.plot(local_accs.mean(0))\n",
    "plt.xlabel(\"iteration\")\n",
    "\n",
    "plt.sca(axs[2])\n",
    "plt.title(\"Global Acceptance\")\n",
    "plt.plot(global_accs.mean(0))\n",
    "plt.xlabel(\"iteration\")\n",
    "\n",
    "plt.sca(axs[3])\n",
    "plt.title(\"Log posterior trace plot\")\n",
    "plt.plot(log_prob_trace)\n",
    "plt.xlabel(\"iteration\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show(block=False)\n",
    "\n",
    "labels = [\"$x_1$\", \"$x_2$\", \"$x_3$\", \"$x_4$\", \"$x_5$\"]\n",
    "# Plot all chains\n",
    "figure = corner.corner(chains.reshape(-1, n_dim), labels=labels)\n",
    "figure.set_size_inches(7, 7)\n",
    "figure.suptitle(\"Visualize samples\")\n",
    "# plt.savefig('dual_moon.png')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `arviz` package to compute $\\hat R$ and the Effective Sample Size (ESS) as a function of the length of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "from arviz.utils import get_coords, _var_names\n",
    "\n",
    "## Load data as arviz InferenceData class\n",
    "idata = az.convert_to_inference_data(chains)\n",
    "coords = {}\n",
    "data = get_coords(az.convert_to_dataset(idata, group=\"posterior\"), coords)\n",
    "var_names = None\n",
    "filter_vars = None\n",
    "var_names = _var_names(var_names, data, filter_vars)\n",
    "n_draws = data.dims[\"draw\"]\n",
    "n_samples = n_draws * data.dims[\"chain\"]\n",
    "first_draw = data.draw.values[0]  # int of where where things should start\n",
    "\n",
    "## Compute where to split the data to diagnostic the convergence\n",
    "n_split = 7\n",
    "xdata = np.linspace(n_samples / n_split, n_samples, n_split)\n",
    "draw_divisions = np.linspace(n_draws // n_split, n_draws, n_split, dtype=int)\n",
    "\n",
    "rhat_s = np.stack(\n",
    "    [\n",
    "        np.array(\n",
    "            az.rhat(\n",
    "                data.sel(draw=slice(first_draw + draw_div)),\n",
    "                var_names=var_names,\n",
    "                method=\"rank\",\n",
    "            )[\"x\"]\n",
    "        )\n",
    "        for draw_div in draw_divisions\n",
    "    ]\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(draw_divisions, rhat_s, \"-o\", label=labels)\n",
    "plt.axhline(1, c=\"k\", ls=\"--\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(r\"$\\hat{R}$\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('manim')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e54ab2e83bcd425a212535c00ec17fb7eaac0eeca07779511da071edfb0d20c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
